{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality\n",
    "\n",
    "Checking the quality of the data used in your analysis is a key step, and often done during an EDA process. But, it can easily be overlooked when moving to production, and even sometimes overlooked by the data engineering team. If there is any doubt that the data your application will be ingesting may have quality control (QC) issues, you should be empowered to set up your own QC process yourself.\n",
    "\n",
    "It is amazing how quickly and easily data can go from being good, high-quality to being junk. A seemingly innocuous change in the schema of one single upstream table can ruin all downstream tables and applications. It is important to remember that, especially in large organizations, there may be multiple teams with ownership of multiple sources of data, and not all of these teams will have disciplined and knowledgeable engineers working on them. \n",
    "\n",
    "The most important data sources to test for quality will typically be the raw sources of data as it comes in, but you can test whichever data sources you have access to since you may not have certain privileges. You may even feel inclined to create tests further downstream to ensure that whatever preprocessing, feature generation, model scoring, and postprocessing code is written is resulting in *expected* data types, shapes, sizes, values and distributions. \n",
    "\n",
    "## Great Expectations\n",
    "\n",
    "[Great Expectations](https://greatexpectations.io/) is a popular, free, and mature data quality tool which allows you to easiy write tests, or *expectations*, using python. From the website, you can use it to validate, document and profile your data. Great Expecations does NOT do data versioning or orchestrate data pipelines.\n",
    "\n",
    "The high-level steps for using Great Expectations are: (a) install Great Expectations; (b) create and configure a Data Context; (c) create your expectations (tests); and (d) validate your data. However, let's start simpler and use the great expectations python library.\n",
    "\n",
    "Let's install Great Expectations, within our virtual environment, with \n",
    "\n",
    "`pip install great_expectations`  \n",
    "\n",
    "and add it to our requirements.txt file. From here, check out the help files:\n",
    "\n",
    "`great_expectations --help`  \n",
    "\n",
    "You'll see that, thankfully, there are only a handful of commands, though each command has many optional arguments, but we' won't worry about these right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eviljimmy/opt/anaconda3/envs/mlops/lib/python3.9/site-packages/great_expectations/optional_imports.py:48: UserWarning: SQLAlchemy v2.0.0 or later is not yet supported by Great Expectations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country       y  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "\n",
    "# First, I'm going to add a header to my data and resave it. It is annoying not having a header.\n",
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'y']\n",
    "path = \"../data/\"\n",
    "train = pd.read_csv(f\"{path}adult.data\", names = col_names)\n",
    "test = pd.read_csv(f\"{path}adult.test\", names = col_names)\n",
    "\n",
    "train.to_csv(f\"{path}adult_train.csv\", index = False)\n",
    "test.to_csv(f\"{path}adult_test.csv\", index = False)\n",
    "\n",
    "# Now reload the new data and make sure it looks right\n",
    "train = pd.read_csv(f\"{path}adult_train.csv\")\n",
    "test = pd.read_csv(f\"{path}adult_test.csv\")\n",
    "\n",
    "# Here we are creating two datasets that we can use with Great Expectations\n",
    "train_df = ge.dataset.PandasDataset(train)\n",
    "test_df = ge.dataset.PandasDataset(test)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table-Level Expectations\n",
    "\n",
    "First, let's think of what we expect our entire table to look like. \n",
    "\n",
    "What columns should be there? \n",
    "Do any columns form a unique identifier?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"observed_value\": [\n",
       "      \"age\",\n",
       "      \"workclass\",\n",
       "      \"fnlwgt\",\n",
       "      \"education\",\n",
       "      \"education-num\",\n",
       "      \"marital-status\",\n",
       "      \"occupation\",\n",
       "      \"relationship\",\n",
       "      \"race\",\n",
       "      \"sex\",\n",
       "      \"capital-gain\",\n",
       "      \"capital-loss\",\n",
       "      \"hours-per-week\",\n",
       "      \"native-country\",\n",
       "      \"y\"\n",
       "    ]\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns\n",
    "train_df.expect_table_columns_to_match_ordered_list(\n",
    "    column_list=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'y']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"observed_value\": [\n",
       "      \"age\",\n",
       "      \"workclass\",\n",
       "      \"fnlwgt\",\n",
       "      \"education\",\n",
       "      \"education-num\",\n",
       "      \"marital-status\",\n",
       "      \"occupation\",\n",
       "      \"relationship\",\n",
       "      \"race\",\n",
       "      \"sex\",\n",
       "      \"capital-gain\",\n",
       "      \"capital-loss\",\n",
       "      \"hours-per-week\",\n",
       "      \"native-country\",\n",
       "      \"y\"\n",
       "    ],\n",
       "    \"details\": {\n",
       "      \"mismatched\": [\n",
       "        {\n",
       "          \"Expected Column Position\": 0,\n",
       "          \"Expected\": \"Rage\",\n",
       "          \"Found\": \"age\"\n",
       "        }\n",
       "      ]\n",
       "    }\n",
       "  },\n",
       "  \"success\": false\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change column list to make success = false\n",
    "test_df.expect_table_columns_to_match_ordered_list(\n",
    "    column_list=['Rage', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'y']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-Level Expectations\n",
    "\n",
    "Second, let's think about our individual columns. \n",
    "\n",
    "- What data types should they be?   \n",
    "- Should there be any missing values?  \n",
    "- What are the minimum and maximum values?  \n",
    "- etc.\n",
    "\n",
    "There are many more expectations which you can find [here](https://greatexpectations.io/expectations), and you can customize your own expectations.\n",
    "\n",
    "Go left to right and define expectations for each column. We'll just do our first two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"observed_value\": \"int64\"\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# age\n",
    "## should be integer\n",
    "train_df.expect_column_values_to_be_of_type(column=\"age\", type_=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"observed_value\": \"int64\"\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.expect_column_values_to_be_of_type(column=\"age\", type_=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"element_count\": 32561,\n",
       "    \"unexpected_count\": 0,\n",
       "    \"unexpected_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 0.0,\n",
       "    \"partial_unexpected_list\": []\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Should have no missing values\n",
    "train_df.expect_column_values_to_not_be_null(column=\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"element_count\": 16281,\n",
       "    \"unexpected_count\": 0,\n",
       "    \"unexpected_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 0.0,\n",
       "    \"partial_unexpected_list\": []\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.expect_column_values_to_not_be_null(column=\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"element_count\": 32561,\n",
       "    \"missing_count\": 0,\n",
       "    \"missing_percent\": 0.0,\n",
       "    \"unexpected_count\": 0,\n",
       "    \"unexpected_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 0.0,\n",
       "    \"unexpected_percent_nonmissing\": 0.0,\n",
       "    \"partial_unexpected_list\": []\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# workclass\n",
    "## should be string\n",
    "train_df.expect_column_values_to_be_of_type(column=\"workclass\", type_=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"observed_value\": 9,\n",
       "    \"element_count\": 32561,\n",
       "    \"missing_count\": null,\n",
       "    \"missing_percent\": null\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## should have 9 unique values\n",
    "train_df.expect_column_unique_value_count_to_be_between(column=\"workclass\", min_value=9, max_value=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"observed_value\": 9,\n",
       "    \"element_count\": 16281,\n",
       "    \"missing_count\": null,\n",
       "    \"missing_percent\": null\n",
       "  },\n",
       "  \"success\": true\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## should have 9 unique values\n",
    "test_df.expect_column_unique_value_count_to_be_between(column=\"workclass\", min_value=9, max_value=9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Suite\n",
    "\n",
    "After defining the expectations we can collect them all into a suite and then use the validate method to run them all and show us any expectations that fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": [],\n",
      "  \"meta\": {\n",
      "    \"great_expectations_version\": \"0.16.5\",\n",
      "    \"expectation_suite_name\": \"default\",\n",
      "    \"run_id\": {\n",
      "      \"run_time\": \"2023-04-04T14:07:55.185142-07:00\",\n",
      "      \"run_name\": null\n",
      "    },\n",
      "    \"batch_kwargs\": {\n",
      "      \"ge_batch_id\": \"c3567874-d32c-11ed-a189-dec3b593fada\"\n",
      "    },\n",
      "    \"batch_markers\": {},\n",
      "    \"batch_parameters\": {},\n",
      "    \"validation_time\": \"20230404T210755.185046Z\",\n",
      "    \"expectation_suite_meta\": {\n",
      "      \"great_expectations_version\": \"0.16.5\"\n",
      "    }\n",
      "  },\n",
      "  \"statistics\": {\n",
      "    \"evaluated_expectations\": 5,\n",
      "    \"successful_expectations\": 5,\n",
      "    \"unsuccessful_expectations\": 0,\n",
      "    \"success_percent\": 100.0\n",
      "  },\n",
      "  \"evaluation_parameters\": {},\n",
      "  \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Expectation suite\n",
    "expectation_suite = train_df.get_expectation_suite(discard_failed_expectations=False)\n",
    "print(train_df.validate(expectation_suite=expectation_suite, only_return_failures=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Great Expectations Within a Project\n",
    "\n",
    "Next, let's use Great Expectations more formally rather than simply defining them in a python script. This way, we can set up our testing suites and document them along with our results. First, we need to initialize and configure our Data Context using \n",
    "\n",
    "`great_expectations init`\n",
    "\n",
    "This will create a great_expectations.yml file which will be our main configuration file. We can add this to our git repo, and ignore the rest. Notice there is a new great_expectations folder, and inside that folder is another folder called uncommitted. The 'uncommitted' folder is meant to be ignored by git, and you'll notice has already been added to a .gitignore file in the great_expectations folder.\n",
    "\n",
    "There is some valuable information in the great_expectations.yml file we should take a look at before moving on. \n",
    "\n",
    "#### Create a New Data Source\n",
    "\n",
    "Next, we should tell Great Expectations about our data sources. We can use data from our local filesystem, or data stored in a relational database. We also must tell Great Expectations how we are processing our data, either using pandas or spark. Let's set up our data sources by running:\n",
    "\n",
    "`great_expectations datasource new`\n",
    "\n",
    "In the terminal, we will be asked:  \n",
    "```\n",
    "What data would you like Great Expectations to connect to?  \n",
    "    1. Files on a filesystem (for processing with Pandas or Spark)  \n",
    "    2. Relational database (SQL)  \n",
    "```\n",
    "\n",
    "Select 1 for filesystem. Then we will be asked:  \n",
    "\n",
    "```\n",
    "What are you processing your files with?  \n",
    "1. Pandas  \n",
    "2. PySpark  \n",
    "```\n",
    "\n",
    "Select 1 for Pandas. Lastly, we will be asked:  \n",
    "\n",
    "```\n",
    "Enter the path of the root directory where the data files are stored: \n",
    "```\n",
    "\n",
    "We should enter the path as `data`. A Jupyter notebook will open up automatically which will allow us to configure our data source. We might only need to make one simple change to the notebook, to the `datasource_name` field (we can rename it to whatever makes the most sense). From here, we run each cell in the notebook, and then we can look in the great_expectations.yml file to see that our new data source was added to it.\n",
    "\n",
    "#### Create a New Suite\n",
    "\n",
    "Next, let's create a suite of expectations by running:\n",
    "\n",
    "`great_expectations suite new`\n",
    "\n",
    "In the terminal we will be asked: \n",
    "\n",
    "```\n",
    "How would you like to create your Expectation Suite?  \n",
    "    1. Manually, without interacting with a sample batch of data (default)  \n",
    "    2. Interactively, with a sample batch of data  \n",
    "    3. Automatically, using a profiler  \n",
    "```\n",
    "\n",
    "Select 2 so that a sample of data will be used to test our expectations. Then we will be asked:\n",
    "\n",
    "```\n",
    "Which data asset (accessible by data connector \"default_inferred_data_connector_name\") would you like to use?  \n",
    "```\n",
    "\n",
    "We will select whichever dataset we want to create a suite for. Let's choose adult_train.csv. And lastly, we will be asked to name the suite. After naming the suite, a notebook will open and we can write our expectations directly in the notebook, and run the cells to validate the expectations. \n",
    "\n",
    "Notice that the suite will be saved in the expectations folder as a json file. If we ever need to check which suites we've created, or edit any of our suites, we can simply run \n",
    "\n",
    "`great_expectations suite list`  \n",
    "`great_expectations suite edit suite_name`  \n",
    "\n",
    "One of the nice things about Great Expectations is that it makes it easy to document our expectations and results. We can build and launch our docs in a browser by running:\n",
    "\n",
    "`great_expectations docs build`\n",
    "\n",
    "#### Using the Profiler\n",
    "\n",
    "Rather than manually create all of our expectations ourselves, we can use a User Configerable Profiler to do some of the work for us. A profiler will create a set of very *strict* expectations, expectations that *overfit* our data and go beyond what we would probably want. After the expectation suite is created using the profiler, we can review the expectations and make any adjustments, additions or deletions.\n",
    "\n",
    "We can use the profiler in one of two ways: (1) the CLI; or (2) a .py file. Let's use the CLI by running \n",
    "\n",
    "`great_expectations suite new --profile`\n",
    "\n",
    "We'll be asked to select a data asset for the suite and then name the suite. Let's pick adult_train.csv and name the suite \"adult_train_data\". Great Expectations will launch a notebook that we can then edit. First we should comment out any columns that we don't want **excluded** from the profiler. After that we can run each code cell in the notebook in order to create the expectation suite. \n",
    "\n",
    "After using the profiler, and reviewing the expectations that were created, we should run `great_expectations edit suite adult_train_data` to edit the expectations.\n",
    "\n",
    "#### Checkpoints\n",
    "\n",
    "Checkpoints are used to validate data. Suppose you want to run a checkpoint every time a data pipeline job runs, you can run a checkpoint. We can see what checkpoints exist by running \n",
    "\n",
    "`great_expectations checkpoint list`\n",
    "\n",
    "and create a new one by running \n",
    "\n",
    "`great_expectations checkpoint new {NAME}`\n",
    "\n",
    "This will open up a notebook that you can then edit to configure a checkpoint. Be sure to check that the yaml for the checkpoint contains the expectation suite that you want to use for the checkpoint and the correct data set.\n",
    "\n",
    "After creating a checkpoint you can run it in the command line:\n",
    "\n",
    "`great_expectations checkpoint run {NAME}`  \n",
    "\n",
    "### Great Expectations Summary\n",
    "\n",
    "There is a lot that Great Expectations can do, and we've only scratched the surface, but hopefully you've noticed how *simple* it can be to create, validate, and document tests for your data without having to write your own unit tests and corresponding application. We have not yet shown how to include Great Expectations in your data pipelines - there are integrations with several popular orchestration engines such as Airflow and Prefect. We'll cover these in a future session.\n",
    "\n",
    "Most of what we've done has been from the CLI which launches notebooks that we have to run and edit - it's all very interactive (and admittedly feels a little strange). There are more pythonic ways of using Great Expectations by simply using the library in some python scripts. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Lab\n",
    "\n",
    "## Overview \n",
    "\n",
    "In this lab we will learn about running tests for data quality using Great Expectations.\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this lab is to set up *unit tests* for your project data.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "### Great Expectations \n",
    "\n",
    "There are a few tools out there for data quality, but we will be using Great Expectations since it is the most popular. Deepchecks is another newer tool that checks more than just the data, it also checks model outputs.\n",
    "\n",
    "Remember the high-level steps for using Great Expectations are: (a) install Great Expectations; (b) create and configure a Data Context; (c) create your expectations (tests); and (d) validate your data.\n",
    "\n",
    "Before beginning, be aware of the data set that you will be checking for quality. In a real job scenario, you should absolutely have checks for every column, but since I am not paying you, do not spend hours and hours doing this for this assignment. If your data has more than five columns, then you can choose the five **most important** features and create expectations just for those five, and then use a profiler for the rest. \n",
    "\n",
    "#### Install Great Expectations, Create and Configure a Data Context\n",
    "\n",
    "1. Install Great Expectations (in your virtual environment): `pip install great_expectations`. You can also add it to your requirements.txt file.   \n",
    "2. Create a Data Context by initializing Great Expectations: `great_expectations init`  \n",
    "3. The great_expectations.yml file can be added to git, but everything else can be ignored, especially everything in the 'uncommitted' folder.  \n",
    "4. Look over the great_expectations.yml file.  \n",
    "5. Create a data source: `great_expectations datasource new`  \n",
    "\n",
    "#### Create Expectations\n",
    "\n",
    "1. Create a new suite of expectations for your data sets: `great_expectations suite new`.    \n",
    "    - If you have more than five columns, create expectations only for the **top five** columns.  \n",
    "\n",
    "#### Validate Data\n",
    "\n",
    "1. Run the cells in the notebook that contains your suite of expectations.  \n",
    "    - Fix any data issues that are revealed by your suite.    \n",
    "2. Take a look at the docs: `great_expectations docs build`.  \n",
    "\n",
    "#### Profile Data\n",
    "\n",
    "1. Create a new suite of expectations for your data sets using a User Configurable Profiler.  \n",
    "    - Edit this suite down to only the essential expectations needed for you data.  \n",
    "2. You should now have two suites of expectations for your data. At this point you can combine expectations into one suite for each set of data.\n",
    "\n",
    "### Final Project\n",
    "\n",
    "For the final project, after you've completed the above steps, repeat those steps using at least one other tool. A couple of other tools you can look at are [Deepchecks](https://deepchecks.com/) and [Pandera](https://pandera.readthedocs.io/en/stable/index.html#).  \n",
    "\n",
    "Once you've worked with other tools, make a comparison. List out pros and cons of each. Think about user-friendliness, costs, integrations with other tools, etc.. You **do not** need to decide which tool you'd pick for your stack yet, because that may depend on the other tools you choose for the rest of the pipeline. Just be sure to write up a good comparison, with evidence, so that you can include it in your final presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b6caa3a4fa55991f3f19a27043e990c8713d062eed39c4a751090dba34a46db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
